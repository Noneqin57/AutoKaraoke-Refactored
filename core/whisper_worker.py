# -*- coding: utf-8 -*-
import os
import re
import gc
import torch
import traceback
import stable_whisper
from multiprocessing import Queue, Event
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any

from config import MIN_DURATION
from core.lrc_parser import LrcParser
from core.lrc_aligner import LrcAligner
from utils.time_utils import format_time
from utils.logger import setup_logger

try:
    import faster_whisper
    HAS_FASTER_WHISPER = True
except ImportError:
    HAS_FASTER_WHISPER = False

@dataclass
class WorkerArgs:
    audio_path: str
    model_size: str
    language: str
    ref_text: str
    lrc_parser_data: Dict[str, Any]
    time_offset: float
    initial_prompt_input: str
    model_dir: str = None
    release_vram: bool = True
    lrc_timestamps: List[float] = field(default_factory=list) # ä¼ é€’è¡Œæ—¶é—´æˆ³åˆ—è¡¨
    enable_force_calibration: bool = True
    enable_avg_distribution: bool = False

def get_attr(obj, key, default=None):
    if isinstance(obj, dict): return obj.get(key, default)
    return getattr(obj, key, default)

def preprocess_cjk_spaces(text):
    if not text: return text
    pattern = r'([\u4e00-\u9fa5\u3040-\u309f\u30a0-\u30ff])'
    spaced = re.sub(pattern, r' \1 ', text)
    return re.sub(r'\s+', ' ', spaced).strip()

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æ¨¡å‹ï¼ˆä»…åœ¨å­è¿›ç¨‹å†…æœ‰æ•ˆï¼‰
_cached_model = None
_cached_model_size = None

def clear_vram(model, force=True):
    global _cached_model, _cached_model_size
    if not force:
        return

    logger = setup_logger("Worker")
    try:
        if model:
            if hasattr(model, 'to'):
                model.to("cpu")
            del model
    except (AttributeError, RuntimeError) as e:
        logger.debug(f"Expected error during VRAM cleanup: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during VRAM cleanup: {e}")
    
    _cached_model = None
    _cached_model_size = None
    
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()

def daemon_worker(input_queue: Queue, result_queue: Queue, progress_queue: Queue, stop_event: Event):
    """
    å¸¸é©»åå°çš„å·¥ä½œè¿›ç¨‹ï¼Œç›‘å¬ä»»åŠ¡é˜Ÿåˆ—å¹¶æ‰§è¡Œ
    """
    global _cached_model, _cached_model_size
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logger("WorkerDaemon")
    logger.info("Daemon worker process started, waiting for tasks...")
    
    while True:
        try:
            task = input_queue.get()
            
            if task == "EXIT":
                logger.info("Received EXIT signal. Shutting down daemon.")
                break
                
            if isinstance(task, WorkerArgs):
                logger.info("Received new task.")
                # é‡ç½® stop_event
                stop_event.clear()
                # æ‰§è¡Œä»»åŠ¡
                run_inference_task(task, result_queue, progress_queue, stop_event)
                
                # ä»»åŠ¡ç»“æŸåï¼Œä¸»åŠ¨è¿›è¡Œä¸€æ¬¡è½»é‡çº§ GCï¼Œä½†ä¿ç•™æ¨¡å‹
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
        except Exception as e:
            logger.error(f"Daemon loop error: {traceback.format_exc()}")
            # é˜²æ­¢æ­»å¾ªç¯ï¼Œç¨ä½œä¼‘çœ 
            import time
            time.sleep(1)

def run_inference_task(args: WorkerArgs, result_queue: Queue, progress_queue: Queue, stop_event: Event):
    """
    æ‰§è¡Œå•æ¬¡æ¨ç†ä»»åŠ¡ (åŸ worker_process é€»è¾‘)
    """
    global _cached_model, _cached_model_size
    
    # åˆå§‹åŒ–æ—¥å¿— (æ¯æ¬¡ä»»åŠ¡å¯èƒ½éœ€è¦æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ global logger)
    logger = setup_logger("Worker")
    
    # Unpack args
    audio_path = args.audio_path
    model_size = args.model_size
    language = args.language
    ref_text = args.ref_text
    lrc_parser_data = args.lrc_parser_data
    time_offset = args.time_offset
    initial_prompt_input = args.initial_prompt_input
    model_dir = args.model_dir or os.path.join(os.getcwd(), "models")
    release_vram_flag = args.release_vram

    try:
        logger.info(f"Worker started. Audio: {audio_path}, Model: {model_size}")
        
        parser = LrcParser()
        parser.headers = lrc_parser_data.get('headers', [])
        parser.lines_text = lrc_parser_data.get('lines_text', [])
        parser.translations = lrc_parser_data.get('translations', {})
        parser.lines_timestamps = args.lrc_timestamps # æ¢å¤æ—¶é—´æˆ³ä¿¡æ¯
        
        local_model_path = model_dir
        os.makedirs(local_model_path, exist_ok=True)
        
        # è®°å½•ä¼ å…¥çš„æ—¶é—´æˆ³ä¿¡æ¯
        if parser.lines_timestamps:
            valid_ts_count = sum(1 for t in parser.lines_timestamps if t > 0)
            logger.info(f"Received timestamps for {valid_ts_count} lines out of {len(parser.lines_timestamps)}")
            # Log first 5 valid timestamps for debugging
            first_few = [t for t in parser.lines_timestamps if t > 0][:5]
            if first_few:
                logger.info(f"First 5 valid timestamps: {first_few}")
        else:
            logger.info("No timestamps received from parser.")
        
        is_cuda = torch.cuda.is_available()
        device = "cuda" if is_cuda else "cpu"
        progress_queue.put(f"âš™ï¸ è¿è¡Œè®¾å¤‡: {device.upper()}")
        logger.info(f"Device: {device}")

        model = None
        
        # å°è¯•ä½¿ç”¨ç¼“å­˜æ¨¡å‹
        if _cached_model:
            if _cached_model_size == model_size:
                logger.info("Using cached model.")
                model = _cached_model
                progress_queue.put(f"âš¡ ä½¿ç”¨ç¼“å­˜æ¨¡å‹ ({model_size})")
            else:
                logger.info(f"Model mismatch (cached: {_cached_model_size}, req: {model_size}). Clearing cache.")
                progress_queue.put("ğŸ”„ åˆ‡æ¢æ¨¡å‹ä¸­ï¼Œé‡Šæ”¾æ—§æ¨¡å‹æ˜¾å­˜...")
                # æ˜¾å¼æ¸…ç†æ—§æ¨¡å‹ï¼Œé˜²æ­¢åŒå€æ˜¾å­˜å ç”¨å¯¼è‡´ OOM
                clear_vram(_cached_model, force=True)
                _cached_model = None
                _cached_model_size = None
        
        # åŠ è½½æ–°æ¨¡å‹
        if not model:
            try:
                use_faster = False
                if HAS_FASTER_WHISPER and not stop_event.is_set():
                    progress_queue.put(f"ğŸš€ åŠ è½½ Faster-Whisper ({model_size})...")
                    progress_queue.put("PROGRESS:10")
                    try:
                        model = stable_whisper.load_faster_whisper(
                            model_size, download_root=local_model_path, device=device,
                            compute_type="float16" if device=="cuda" else "int8"
                        )
                        use_faster = True
                    except Exception as fw_error:
                        logger.warning(f"Faster-Whisper load failed: {fw_error}")
                        model = None
                
                if not model and not stop_event.is_set():
                    progress_queue.put(f"åŠ è½½æ ‡å‡†æ¨¡å‹ ({model_size})...")
                    progress_queue.put("PROGRESS:10")
                    model = stable_whisper.load_model(model_size, download_root=local_model_path, device=device)
                
                # æ›´æ–°ç¼“å­˜
                if not release_vram_flag:
                    _cached_model = model
                    _cached_model_size = model_size
                    
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            
        # è¯­è¨€å‚æ•°å¤„ç†
        lang_param = language 
        # ç§»é™¤ Auto æ£€æµ‹é€»è¾‘ï¼Œå› ä¸º UI å·²ç»å¼ºåˆ¶é€‰æ‹©äº†è¯­è¨€
        
        progress_queue.put("PROGRESS:30")
        result = None
        if stop_event.is_set():
            result_queue.put(("aborted", None))
            return
        
        if ref_text and ref_text.strip():
            progress_queue.put("æ­£åœ¨è¿›è¡Œã€ç»“æ„åŒ–å¼ºåˆ¶å¯¹é½ã€‘...")
            spaced_ref_text = preprocess_cjk_spaces(ref_text)
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°è°ƒç”¨ align
            # æ³¨æ„: vad=True éœ€è¦ä¸‹è½½ Silero VAD æ¨¡å‹ï¼Œå¦‚æœç½‘ç»œä¸é€šä¼šå¯¼è‡´ 502/ConnectTimeout
            # è¿™é‡Œæˆ‘ä»¬å…ˆç¦ç”¨ vad å‚æ•°ä»¥ç¡®ä¿å›½å†…ç½‘ç»œä¸‹çš„ç¨³å®šæ€§ï¼Œ
            # ä¾é  suppress_silence å’Œå…¨å±€å¯¹é½ç®—æ³•æ¥å¤„ç†é™éŸ³ã€‚
            align_args = {
                "language": lang_param, 
                "suppress_silence": True, 
                "regroup": False
            }
            
            # åªæœ‰å½“ç¡®å®å·²ç»ä¸‹è½½äº† VAD æ¨¡å‹æˆ–è€…ç½‘ç»œç¯å¢ƒå…è®¸æ—¶æ‰å»ºè®®å¼€å¯ vad=True
            # result = model.align(audio_path, spaced_ref_text, **align_args)
            
            # å¦‚æœæ˜¯ faster-whisperï¼Œalign æ–¹æ³•å‚æ•°å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œä½† stable-whisper åšäº†å°è£…
            result = model.align(audio_path, spaced_ref_text, **align_args)
        else:
            progress_queue.put("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
            transcribe_args = {"language": lang_param, "word_timestamps": True, "vad": True, "regroup": False}
            if initial_prompt_input and initial_prompt_input.strip():
                transcribe_args["initial_prompt"] = initial_prompt_input.strip()
            if hasattr(model, "model") and "FasterWhisper" in str(type(model.model)): # Check if faster whisper
                 transcribe_args["beam_size"] = 5
            
            result = model.transcribe(audio_path, **transcribe_args)
        
        if stop_event.is_set():
            result_queue.put(("aborted", None))
            return
        
        progress_queue.put("æ­£åœ¨åˆæˆç»“æœ...")
        progress_queue.put("PROGRESS:90")
        
        aligner = LrcAligner(
            parser, 
            time_offset, 
            enable_force_calibration=args.enable_force_calibration,
            enable_avg_distribution=args.enable_avg_distribution
        )
        lrc_content = aligner.run(result, stop_event, progress_queue)
        
        if stop_event.is_set():
            result_queue.put(("aborted", None))
        else:
            result_queue.put(("success", lrc_content))
            progress_queue.put("PROGRESS:100")
            logger.info("Task completed successfully.")

    except torch.cuda.OutOfMemoryError:
        logger.error("OOM Error")
        result_queue.put(("error", "âŒ æ˜¾å­˜ä¸è¶³ï¼è¯·å°è¯•æ›´å°çš„æ¨¡å‹"))
        clear_vram(model, force=True) # OOM æ—¶å¼ºåˆ¶æ¸…ç†
    except Exception as e:
        if not stop_event.is_set():
            logger.error(f"Error: {traceback.format_exc()}")
            result_queue.put(("error", f"é”™è¯¯: {str(e)}"))
    finally:
        # æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦é‡Šæ”¾æ˜¾å­˜
        if release_vram_flag:
            clear_vram(model, force=True)
        else:
            # å¦‚æœä¿ç•™æ˜¾å­˜ï¼Œä¸åšä»»ä½•æ“ä½œï¼Œè®© _cached_model ä¿æŒå¼•ç”¨
            pass
