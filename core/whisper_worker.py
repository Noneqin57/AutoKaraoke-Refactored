# -*- coding: utf-8 -*-
import os
import re
import gc
import torch
import traceback
import stable_whisper
from multiprocessing import Queue, Event
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any

from config import MIN_DURATION
from core.lrc_parser import LrcParser
from core.lrc_aligner import LrcAligner
from utils.time_utils import format_time
from utils.logger import setup_logger

try:
    import faster_whisper
    HAS_FASTER_WHISPER = True
except ImportError:
    HAS_FASTER_WHISPER = False

@dataclass
class WorkerArgs:
    audio_path: str
    model_size: str
    language: str
    ref_text: str
    lrc_parser_data: Dict[str, Any]
    time_offset: float
    initial_prompt_input: str
    model_dir: str = None
    release_vram: bool = True
    lrc_timestamps: List[float] = field(default_factory=list) # ä¼ é€’è¡Œæ—¶é—´æˆ³åˆ—è¡¨
    enable_force_calibration: bool = True
    enable_avg_distribution: bool = False

def get_attr(obj, key, default=None):
    if isinstance(obj, dict): return obj.get(key, default)
    return getattr(obj, key, default)

def preprocess_cjk_spaces(text):
    if not text: return text
    pattern = r'([\u4e00-\u9fa5\u3040-\u309f\u30a0-\u30ff])'
    spaced = re.sub(pattern, r' \1 ', text)
    return re.sub(r'\s+', ' ', spaced).strip()

class ModelCache:
    """æ¨¡å‹ç¼“å­˜ç®¡ç†ç±»ï¼Œå°è£…å…¨å±€æ¨¡å‹çŠ¶æ€"""
    
    def __init__(self):
        self.model = None
        self.model_size = None
        self.logger = setup_logger("ModelCache")
    
    def get(self):
        """è·å–ç¼“å­˜çš„æ¨¡å‹"""
        return self.model, self.model_size
    
    def set(self, model, model_size):
        """è®¾ç½®ç¼“å­˜çš„æ¨¡å‹"""
        self.model = model
        self.model_size = model_size
        self.logger.info(f"Model cached: {model_size}")
    
    def clear(self, force=True):
        """æ¸…ç†æ˜¾å­˜å¹¶é‡ç½®ç¼“å­˜"""
        if not force:
            return
        
        try:
            if self.model:
                if hasattr(self.model, 'to'):
                    self.model.to("cpu")
                del self.model
        except (AttributeError, RuntimeError) as e:
            self.logger.debug(f"Expected error during VRAM cleanup: {e}")
        except Exception as e:
            self.logger.error(f"Unexpected error during VRAM cleanup: {e}")
        
        self.model = None
        self.model_size = None
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def is_cached(self, model_size):
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
        return self.model is not None and self.model_size == model_size


# å…¨å±€æ¨¡å‹ç¼“å­˜å®ä¾‹
_model_cache = ModelCache()

def daemon_worker(input_queue: Queue, result_queue: Queue, progress_queue: Queue, stop_event: Event):
    """å¸¸é©»åå°çš„å·¥ä½œè¿›ç¨‹ï¼Œç›‘å¬ä»»åŠ¡é˜Ÿåˆ—å¹¶æ‰§è¡Œ"""
    global _model_cache
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logger("WorkerDaemon")
    logger.info("Daemon worker process started, waiting for tasks...")
    
    while True:
        try:
            task = input_queue.get()
            
            if task == "EXIT":
                logger.info("Received EXIT signal. Shutting down daemon.")
                _model_cache.clear(force=True)
                break
                
            if isinstance(task, WorkerArgs):
                logger.info("Received new task.")
                stop_event.clear()
                
                # æ‰§è¡Œä»»åŠ¡
                run_inference_task(task, result_queue, progress_queue, stop_event)
                
                # ä»»åŠ¡ç»“æŸåè¿›è¡Œè½»é‡çº§æ¸…ç†ï¼Œä½†ä¿ç•™æ¨¡å‹
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
        except Exception as e:
            logger.error(f"Daemon loop error: {traceback.format_exc()}")
            # é˜²æ­¢æ­»å¾ªç¯ï¼Œç¨ä½œä¼‘çœ 
            import time
            time.sleep(1)

def run_inference_task(args: WorkerArgs, result_queue: Queue, progress_queue: Queue, stop_event: Event):
    """æ‰§è¡Œå•æ¬¡æ¨ç†ä»»åŠ¡"""
    global _model_cache
    
    logger = setup_logger("Worker")
    
    # è§£åŒ…å‚æ•°
    audio_path = args.audio_path
    model_size = args.model_size
    language = args.language
    ref_text = args.ref_text
    lrc_parser_data = args.lrc_parser_data
    time_offset = args.time_offset
    initial_prompt_input = args.initial_prompt_input
    model_dir = args.model_dir or os.path.join(os.getcwd(), "models")
    release_vram_flag = args.release_vram

    try:
        logger.info(f"Worker started. Audio: {audio_path}, Model: {model_size}")
        
        # æ¢å¤è§£æå™¨çŠ¶æ€
        parser = LrcParser()
        parser.headers = lrc_parser_data.get('headers', [])
        parser.lines_text = lrc_parser_data.get('lines_text', [])
        parser.translations = lrc_parser_data.get('translations', {})
        parser.lines_timestamps = args.lrc_timestamps
        
        local_model_path = model_dir
        os.makedirs(local_model_path, exist_ok=True)
        
        # è®°å½•æ—¶é—´æˆ³ä¿¡æ¯
        if parser.lines_timestamps:
            valid_ts_count = sum(1 for t in parser.lines_timestamps if t > 0)
            logger.info(f"Received timestamps for {valid_ts_count} lines out of {len(parser.lines_timestamps)}")
            first_few = [t for t in parser.lines_timestamps if t > 0][:5]
            if first_few:
                logger.info(f"First 5 valid timestamps: {first_few}")
        else:
            logger.info("No timestamps received from parser.")
        
        is_cuda = torch.cuda.is_available()
        device = "cuda" if is_cuda else "cpu"
        progress_queue.put(f"âš™ï¸ è¿è¡Œè®¾å¤‡: {device.upper()}")
        logger.info(f"Device: {device}")

        model = None
        
        # ä½¿ç”¨æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨
        if _model_cache.is_cached(model_size):
            logger.info("Using cached model.")
            model, _ = _model_cache.get()
            progress_queue.put(f"âš¡ ä½¿ç”¨ç¼“å­˜æ¨¡å‹ ({model_size})")
        else:
            cached_model, cached_size = _model_cache.get()
            if cached_model:
                logger.info(f"Model mismatch (cached: {cached_size}, req: {model_size}). Clearing cache.")
                progress_queue.put("ğŸ”„ åˆ‡æ¢æ¨¡å‹ä¸­ï¼Œé‡Šæ”¾æ—§æ¨¡å‹æ˜¾å­˜...")
                _model_cache.clear(force=True)
        
        # åŠ è½½æ–°æ¨¡å‹
        if not model:
            try:
                use_faster = False
                if HAS_FASTER_WHISPER and not stop_event.is_set():
                    progress_queue.put(f"ğŸš€ åŠ è½½ Faster-Whisper ({model_size})...")
                    progress_queue.put("PROGRESS:10")
                    try:
                        # å°è¯•ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
                        faster_whisper_path = os.path.join(local_model_path, f"faster-whisper-{model_size}")
                        if os.path.exists(faster_whisper_path):
                            logger.info(f"Loading from local path: {faster_whisper_path}")
                            model = stable_whisper.load_faster_whisper(
                                faster_whisper_path, device=device,
                                compute_type="float16" if device=="cuda" else "int8"
                            )
                        else:
                            # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼Œè®© stable_whisper è‡ªåŠ¨ä¸‹è½½
                            logger.info(f"Local model not found, falling back to auto-download")
                            model = stable_whisper.load_faster_whisper(
                                model_size, download_root=local_model_path, device=device,
                                compute_type="float16" if device=="cuda" else "int8"
                            )
                        use_faster = True
                    except Exception as fw_error:
                        logger.warning(f"Faster-Whisper load failed: {fw_error}")
                        model = None
                
                if not model and not stop_event.is_set():
                    progress_queue.put(f"åŠ è½½æ ‡å‡†æ¨¡å‹ ({model_size})...")
                    progress_queue.put("PROGRESS:10")
                    model = stable_whisper.load_model(model_size, download_root=local_model_path, device=device)
                
                # æ›´æ–°ç¼“å­˜
                if not release_vram_flag:
                    _model_cache.set(model, model_size)
                    
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            
        # è¯­è¨€å‚æ•°å¤„ç†
        lang_param = language 
        # ç§»é™¤ Auto æ£€æµ‹é€»è¾‘ï¼Œå› ä¸º UI å·²ç»å¼ºåˆ¶é€‰æ‹©äº†è¯­è¨€
        
        progress_queue.put("PROGRESS:30")
        result = None
        if stop_event.is_set():
            result_queue.put(("aborted", None))
            return
        
        if ref_text and ref_text.strip():
            progress_queue.put("æ­£åœ¨è¿›è¡Œã€ç»“æ„åŒ–å¼ºåˆ¶å¯¹é½ã€‘...")
            spaced_ref_text = preprocess_cjk_spaces(ref_text)
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°è°ƒç”¨ align
            # æ³¨æ„: vad=True éœ€è¦ä¸‹è½½ Silero VAD æ¨¡å‹ï¼Œå¦‚æœç½‘ç»œä¸é€šä¼šå¯¼è‡´ 502/ConnectTimeout
            # è¿™é‡Œæˆ‘ä»¬å…ˆç¦ç”¨ vad å‚æ•°ä»¥ç¡®ä¿å›½å†…ç½‘ç»œä¸‹çš„ç¨³å®šæ€§ï¼Œ
            # ä¾é  suppress_silence å’Œå…¨å±€å¯¹é½ç®—æ³•æ¥å¤„ç†é™éŸ³ã€‚
            align_args = {
                "language": lang_param, 
                "suppress_silence": True, 
                "regroup": False
            }
            
            # åªæœ‰å½“ç¡®å®å·²ç»ä¸‹è½½äº† VAD æ¨¡å‹æˆ–è€…ç½‘ç»œç¯å¢ƒå…è®¸æ—¶æ‰å»ºè®®å¼€å¯ vad=True
            # result = model.align(audio_path, spaced_ref_text, **align_args)
            
            # å¦‚æœæ˜¯ faster-whisperï¼Œalign æ–¹æ³•å‚æ•°å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œä½† stable-whisper åšäº†å°è£…
            result = model.align(audio_path, spaced_ref_text, **align_args)
        else:
            progress_queue.put("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
            transcribe_args = {"language": lang_param, "word_timestamps": True, "vad": True, "regroup": False}
            if initial_prompt_input and initial_prompt_input.strip():
                transcribe_args["initial_prompt"] = initial_prompt_input.strip()
            if hasattr(model, "model") and "FasterWhisper" in str(type(model.model)): # Check if faster whisper
                 transcribe_args["beam_size"] = 5
            
            result = model.transcribe(audio_path, **transcribe_args)
        
        if stop_event.is_set():
            result_queue.put(("aborted", None))
            return
        
        progress_queue.put("æ­£åœ¨åˆæˆç»“æœ...")
        progress_queue.put("PROGRESS:90")
        
        aligner = LrcAligner(
            parser, 
            time_offset, 
            enable_force_calibration=args.enable_force_calibration,
            enable_avg_distribution=args.enable_avg_distribution
        )
        lrc_content = aligner.run(result, stop_event, progress_queue)
        
        if stop_event.is_set():
            result_queue.put(("aborted", None))
        else:
            result_queue.put(("success", lrc_content))
            progress_queue.put("PROGRESS:100")
            logger.info("Task completed successfully.")

    except torch.cuda.OutOfMemoryError:
        logger.error("OOM Error")
        result_queue.put(("error", "âŒ æ˜¾å­˜ä¸è¶³ï¼è¯·å°è¯•æ›´å°çš„æ¨¡å‹"))
        _model_cache.clear(force=True)
    except Exception as e:
        if not stop_event.is_set():
            logger.error(f"Error: {traceback.format_exc()}")
            result_queue.put(("error", f"é”™è¯¯: {str(e)}"))
    finally:
        # æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦é‡Šæ”¾æ˜¾å­˜
        if release_vram_flag:
            _model_cache.clear(force=True)
